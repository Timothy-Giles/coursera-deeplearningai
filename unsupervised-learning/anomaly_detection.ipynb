{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - Anomaly detection\n",
    "\n",
    "2.1 Problem Statement\n",
    "In this exercise, you will implement an anomaly detection algorithm to detect anomalous behavior in server computers.\n",
    "\n",
    "The dataset contains two features -\n",
    "\n",
    "throughput (mb/s) and\n",
    "latency (ms) of response of each server.\n",
    "While your servers were operating, you collected  ğ‘š=307\n",
    "  examples of how they were behaving, and thus have an unlabeled dataset  {ğ‘¥(1),â€¦,ğ‘¥(ğ‘š)}\n",
    " .\n",
    "\n",
    "You suspect that the vast majority of these examples are â€œnormalâ€ (non-anomalous) examples of the servers operating normally, but there might also be some examples of servers acting anomalously within this dataset.\n",
    "You will use a Gaussian model to detect anomalous examples in your dataset.\n",
    "\n",
    "You will first start on a 2D dataset that will allow you to visualize what the algorithm is doing.\n",
    "On that dataset you will fit a Gaussian distribution and then find values that have very low probability and hence can be considered anomalies.\n",
    "After that, you will apply the anomaly detection algorithm to a larger dataset with many dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_val, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "X_train, X_val, y_val = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five elements of X_train\n",
    "print(\"The first 5 elements of X_train are:\\n\", X_train[:5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five elements of X_val\n",
    "print(\"The first 5 elements of X_val are\\n\", X_val[:5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five elements of y_val\n",
    "print(\"The first 5 elements of y_val are\\n\", y_val[:5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The shape of X_train is:', X_train.shape)\n",
    "print ('The shape of X_val is:', X_val.shape)\n",
    "print ('The shape of y_val is: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of the data.\n",
    "# we used the 'marker' and 'c' parameters\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], marker='x', c='b') \n",
    "\n",
    "# Set the title\n",
    "plt.title(\"The first dataset\")\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Throughput (mb/s)')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('Latency (ms)')\n",
    "# Set axis range\n",
    "plt.axis([0, 30, 0, 30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Gaussian distribution\n",
    "To perform anomaly detection, you will first need to fit a model to the dataâ€™s distribution.\n",
    "\n",
    "Given a training set  {ğ‘¥(1),...,ğ‘¥(ğ‘š)}\n",
    "  you want to estimate the Gaussian distribution for each of the features  ğ‘¥ğ‘–\n",
    " .\n",
    "\n",
    "Recall that the Gaussian distribution is given by\n",
    "\n",
    "ğ‘(ğ‘¥;ğœ‡,ğœ2)=12ğœ‹ğœ2â¯â¯â¯â¯â¯â¯â¯â¯âˆšexpâˆ’(ğ‘¥âˆ’ğœ‡)22ğœ2\n",
    " \n",
    "where  ğœ‡\n",
    "  is the mean and  ğœ2\n",
    "  is the variance.\n",
    "\n",
    "For each feature  ğ‘–=1â€¦ğ‘›\n",
    " , you need to find parameters  ğœ‡ğ‘–\n",
    "  and  ğœ2ğ‘–\n",
    "  that fit the data in the  ğ‘–\n",
    " -th dimension  {ğ‘¥(1)ğ‘–,...,ğ‘¥(ğ‘š)ğ‘–}\n",
    "  (the  ğ‘–\n",
    " -th dimension of each example).\n",
    "\n",
    " -------------------------------------------------------------------------------------------------------\n",
    " \n",
    " Exercise 1\n",
    "Please complete the estimate_gaussian function below to calculate mu (mean for each feature in X) and var (variance for each feature in X).\n",
    "\n",
    "You can estimate the parameters, ( ğœ‡ğ‘–\n",
    " ,  ğœ2ğ‘–\n",
    " ), of the  ğ‘–\n",
    " -th feature by using the following equations. To estimate the mean, you will use:\n",
    "\n",
    "ğœ‡ğ‘–=1ğ‘šâˆ‘ğ‘—=1ğ‘šğ‘¥(ğ‘—)ğ‘–\n",
    " \n",
    "and for the variance you will use:\n",
    "ğœ2ğ‘–=1ğ‘šâˆ‘ğ‘—=1ğ‘š(ğ‘¥(ğ‘—)ğ‘–âˆ’ğœ‡ğ‘–)2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_gaussian(X): \n",
    "    \"\"\"\n",
    "    Calculates mean and variance of all features \n",
    "    in the dataset\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): (m, n) Data matrix\n",
    "    \n",
    "    Returns:\n",
    "        mu (ndarray): (n,) Mean of all features\n",
    "        var (ndarray): (n,) Variance of all features\n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    mu = np.mean(X, axis=0) \n",
    "    var = np.var(X, axis=0) \n",
    "            \n",
    "    return mu, var\n",
    "\n",
    "print(\"Mean of each feature:\", mu)\n",
    "print(\"Variance of each feature:\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the density of the multivariate normal\n",
    "# at each data point (row) of X_train\n",
    "p = multivariate_gaussian(X_train, mu, var)\n",
    "\n",
    "#Plotting code \n",
    "visualize_fit(X_train, mu, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2 Selecting the threshold ğœ–\n",
    "Now that you have estimated the Gaussian parameters, you can investigate which examples have a very high probability given this distribution and which examples have a very low probability.\n",
    "\n",
    "The low probability examples are more likely to be the anomalies in our dataset.\n",
    "One way to determine which examples are anomalies is to select a threshold based on a cross validation set.\n",
    "In this section, you will complete the code in select_threshold to select the threshold ğœ€\n",
    " using the ğ¹1\n",
    " score on a cross validation set.\n",
    "\n",
    "For this, we will use a cross validation set {(ğ‘¥(1)cv,ğ‘¦(1)cv),â€¦,(ğ‘¥(ğ‘šcv)cv,ğ‘¦(ğ‘šcv)cv)}\n",
    ", where the label ğ‘¦=1\n",
    " corresponds to an anomalous example, and ğ‘¦=0\n",
    " corresponds to a normal example.\n",
    "For each cross validation example, we will compute ğ‘(ğ‘¥(ğ‘–)cv)\n",
    ". The vector of all of these probabilities ğ‘(ğ‘¥(1)cv),â€¦,ğ‘(ğ‘¥(ğ‘šcv)cv)\n",
    " is passed to select_threshold in the vector p_val.\n",
    "The corresponding labels ğ‘¦(1)cv,â€¦,ğ‘¦(ğ‘šcv)cv\n",
    " are passed to the same function in the vector y_val.\n",
    "\n",
    "Exercise 2\n",
    "Please complete the select_threshold function below to find the best threshold to use for selecting outliers based on the results from the validation set (p_val) and the ground truth (y_val).\n",
    "\n",
    "In the provided code select_threshold, there is already a loop that will try many different values of  ğœ€\n",
    "  and select the best  ğœ€\n",
    "  based on the  ğ¹1\n",
    "  score.\n",
    "\n",
    "You need to implement code to calculate the F1 score from choosing epsilon as the threshold and place the value in F1.\n",
    "\n",
    "Recall that if an example  ğ‘¥\n",
    "  has a low probability  ğ‘(ğ‘¥)<ğœ€\n",
    " , then it is classified as an anomaly.\n",
    "\n",
    " tp is the number of true positives: the ground truth label says itâ€™s an anomaly and our algorithm correctly classified it as an anomaly.\n",
    " fp is the number of false positives: the ground truth label says itâ€™s not an anomaly, but our algorithm incorrectly classified it as an anomaly.\n",
    " fn is the number of false negatives: the ground truth label says itâ€™s an anomaly, but our algorithm incorrectly classified it as not being anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_threshold(y_val, p_val): \n",
    "    \"\"\"\n",
    "    Finds the best threshold to use for selecting outliers \n",
    "    based on the results from a validation set (p_val) \n",
    "    and the ground truth (y_val)\n",
    "    \n",
    "    Args:\n",
    "        y_val (ndarray): Ground truth on validation set\n",
    "        p_val (ndarray): Results on validation set\n",
    "        \n",
    "    Returns:\n",
    "        epsilon (float): Threshold chosen \n",
    "        F1 (float):      F1 score by choosing epsilon as threshold\n",
    "    \"\"\" \n",
    "\n",
    "    best_epsilon = 0\n",
    "    best_F1 = 0\n",
    "    F1 = 0\n",
    "    \n",
    "    step_size = (max(p_val) - min(p_val)) / 1000\n",
    "    \n",
    "    for epsilon in np.arange(min(p_val), max(p_val), step_size):\n",
    "    \n",
    "        predictions = p_val < epsilon\n",
    "\n",
    "        tp = np.sum((predictions == 1) & (y_val == 1))\n",
    "        fp = np.sum((predictions == 1) & (y_val == 0))\n",
    "        fn = np.sum((predictions == 0) & (y_val == 1))\n",
    "\n",
    "        prec = tp / (tp + fp)\n",
    "        rec = tp / (tp + fn)\n",
    "\n",
    "        F1 = 2*prec*rec/(prec+rec)\n",
    "        \n",
    "        if F1 > best_F1:\n",
    "            best_F1 = F1\n",
    "            best_epsilon = epsilon\n",
    "        \n",
    "    return best_epsilon, best_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = multivariate_gaussian(X_val, mu, var)\n",
    "epsilon, F1 = select_threshold(y_val, p_val)\n",
    "\n",
    "print('Best epsilon found using cross-validation: %e' % epsilon)\n",
    "print('Best F1 on Cross Validation Set: %f' % F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the outliers in the training set \n",
    "outliers = p < epsilon\n",
    "\n",
    "# Visualize the fit\n",
    "visualize_fit(X_train, mu, var)\n",
    "\n",
    "# Draw a red circle around those outliers\n",
    "plt.plot(X_train[outliers, 0], X_train[outliers, 1], 'ro',\n",
    "         markersize= 10,markerfacecolor='none', markeredgewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 High dimensional dataset\n",
    "Now, we will run the anomaly detection algorithm that you implemented on a more realistic and much harder dataset.\n",
    "\n",
    "In this dataset, each example is described by 11 features, capturing many more properties of your compute servers.\n",
    "\n",
    "Let's start by loading the dataset.\n",
    "\n",
    "The load_data() function shown below loads the data into variables X_train_high, X_val_high and y_val_high\n",
    "_high is meant to distinguish these variables from the ones used in the previous part\n",
    "We will use X_train_high to fit Gaussian distribution\n",
    "We will use X_val_high and y_val_high as a cross validation set to select a threshold and determine anomalous vs normal examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "X_train_high, X_val_high, y_val_high = load_data_multi()\n",
    "\n",
    "print ('The shape of X_train_high is:', X_train_high.shape)\n",
    "print ('The shape of X_val_high is:', X_val_high.shape)\n",
    "print ('The shape of y_val_high is: ', y_val_high.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same steps to the larger dataset\n",
    "\n",
    "# Estimate the Gaussian parameters\n",
    "mu_high, var_high = estimate_gaussian(X_train_high)\n",
    "\n",
    "# Evaluate the probabilites for the training set\n",
    "p_high = multivariate_gaussian(X_train_high, mu_high, var_high)\n",
    "\n",
    "# Evaluate the probabilites for the cross validation set\n",
    "p_val_high = multivariate_gaussian(X_val_high, mu_high, var_high)\n",
    "\n",
    "# Find the best threshold\n",
    "epsilon_high, F1_high = select_threshold(y_val_high, p_val_high)\n",
    "\n",
    "print('Best epsilon found using cross-validation: %e'% epsilon_high)\n",
    "print('Best F1 on Cross Validation Set:  %f'% F1_high)\n",
    "print('# Anomalies found: %d'% sum(p_high < epsilon_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
